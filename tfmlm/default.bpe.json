{
    "optimizer": "lazyadam",
    "optimizer_list": ["adagrad", "adam", "lazyadam"],
    
    "learning_rate_schedule_list": [
        "constant",
        "warmup_cosine"
        ],
    "adagrad": {
        "learning_rate_schedule": "constant",
        "max_learning_rate": 0.2,
        "min_learning_rate": 0.0
    },
    "adam": {
        "beta1": 0.9,
        "beta2": 0.98,
        "epsilon": 1e-9,
        "learning_rate_schedule": "warmup_cosine",
        "max_learning_rate": 0.2,
        "min_learning_rate": 0.0,
        "warmup_steps": 2000
    },
    "lazyadam": {
        "beta1": 0.9,
        "beta2": 0.98,
        "epsilon": 1e-9,
        "learning_rate_schedule": "warmup_cosine",
        "max_learning_rate": 0.02,
        "min_learning_rate": 0.0,
        "warmup_steps": 100
    },
    "lstm": {
        "use_skip_connections": true,
        "dim": 4096,
        "n_layers": 2,
        "proj_clip": 3,
        "cell_clip": 3,
        "projection_dim": 512
    },
    "shuffle_training_data": false,
    "sample_softmax": false,
    "store_embedding_in_gpu": true,
    "dropout": 0.2,
    "n_train_tokens": 768648884,
    "bidirectional": true,
    "use_transformer": true,
    "n_tokens_vocab": 793471,
    "n_epochs": 30,
    "n_negative_samples_batch": 8192,
    "all_clip_norm_val": 10.0,
    "num_context_steps": 64,
    "batch_size": 8,
    "transformer": {
        "hidden_size": 512,
        "residual_dropout": 0.2,
        "attention_dropout": 0.2,
        "layer_preprocess": "layer_norm",
        "num_decoder_layers": 8,
        "num_heads": 8,
        "max_relative_dist": 64,
        "filter_size": 2048,
        "relu_dropout": 0.2,
        "no_additional_dropout": true
    },
    "scale_embeddings": true,
    "share_embedding_softmax": true,
    "unroll_steps": 384
}
